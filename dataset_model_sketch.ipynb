{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data imports\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# model imports\n",
    "# similar to https://github.com/huggingface/transformers/blob/14e9d2954c3a7256a49a3e581ae25364c76f521e/src/transformers/models/bert/modeling_bert.py\n",
    "import logging\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss, HingeEmbeddingLoss\n",
    "\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers.models.bert.modeling_bert import BertEmbeddings, BertModel, BertPreTrainedModel\n",
    "from transformers.utils import logging\n",
    "\n",
    "from transformers.file_utils import ModelOutput\n",
    "from typing import Optional\n",
    "\n",
    "# logger = logging.get_logger(__name__)\n",
    "\n",
    "# Trainer imports\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each training instance consists of a paragraph, edu splits and edu labels\n",
    "\n",
    "class ArgumentDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, tokenizer, paragraph_files, edus_files, labels_files, max_len=128, max_edu_seq=50):\n",
    "        \n",
    "        self.max_len, self.max_edu_seq = max_len, max_edu_seq\n",
    "        self.tokenizer = tokenizer\n",
    "        self.paragraphs = [''.join(open(file).readlines()) for file in glob.glob(paragraph_files)]\n",
    "        self.edus = [open(file).readlines() for file in glob.glob(edus_files)]\n",
    "        self.labels = [open(file).readlines() for file in glob.glob(labels_files)]\n",
    "        self.label2id = {'B-claim': 1, 'I-claim': 2, 'B-premise': 3, 'I-premise': 4, 'O' : 0}\n",
    "        \n",
    "        ######\n",
    "        filterout = [7, 24, 89, 231, 298, 348, 370, 373, 421, 473, 481, 485, 496, 508, 599, 680]\n",
    "        for i in filterout[::-1]:\n",
    "            self.paragraphs.pop(i); self.edus.pop(i); self.labels.pop(i)\n",
    "        # print(len(self.paragraphs), len(self.edus), len(self.labels))\n",
    "        ######\n",
    "        \n",
    "        self.labels = [\n",
    "            [{'edu': line.rstrip().split('\\t')[0], 'tokens': line.rstrip().split('\\t')[1]} for line in para_labels]\n",
    "                      for para_labels in self.labels\n",
    "        ]\n",
    "        self.edus_tokenized = [self.tokenizer(para_edus, truncation=True, padding='max_length', max_length=self.max_len) for para_edus in self.edus]       \n",
    "        self.edus_tokenized2 = self.tokenizer.batch_encode_plus(self.edus[0], padding='max_length', max_length=self.max_len)\n",
    "        \n",
    "        self.edu_seq_input_ids = torch.full((len(self.edus), self.max_edu_seq, self.max_len), 0)\n",
    "        self.edu_seq_attention_mask = torch.full((len(self.edus), self.max_edu_seq, self.max_len), 0)\n",
    "        self.edu_seq_token_type_ids = torch.full((len(self.edus), self.max_edu_seq, self.max_len), 0)\n",
    "        self.label_edus = [[0 for _ in range(self.max_edu_seq)] for _ in self.labels]\n",
    "        self.label_tokens = [[[0 for _ in range(self.max_len)] for _ in range(self.max_edu_seq)] for _ in self.labels]\n",
    "        \n",
    "        for i, para_edus in enumerate(self.edus_tokenized):\n",
    "            for j in range(min(self.max_edu_seq, len(para_edus['input_ids']))):\n",
    "                self.edu_seq_input_ids[i][j] = torch.tensor(para_edus['input_ids'][j])\n",
    "                self.edu_seq_attention_mask[i][j] = torch.tensor(para_edus['attention_mask'][j])\n",
    "                self.edu_seq_token_type_ids[i][j] = torch.tensor(para_edus['token_type_ids'][j])\n",
    "                \n",
    "        for i, para_edus in enumerate(self.labels):\n",
    "            for j in range(min(self.max_edu_seq, len(self.labels[i]))):\n",
    "                self.label_edus[i][j] = self.label2id[self.labels[i][j]['edu']]\n",
    "                for k in range(min(self.max_len, len(self.labels[i][j]['tokens'].split()))):\n",
    "                    self.label_tokens[i][j][k] = self.label2id[self.labels[i][j]['tokens'].split()[k]]\n",
    "        \n",
    "        # self.paragraphs_tokenized = self.tokenizer(self.paragraphs, truncation=True, padding='max_length', max_length=512)\n",
    "        # self.paragraphs_tokenized = [self.tokenizer.tokenize(p, truncation=True, padding='max_length', max_length=128) for p in self.paragraphs]\n",
    "        # assert len(self.paragraphs) == len(self.edus) == len(self.labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return { 'edu_seq_input_ids' : self.edu_seq_input_ids[i],\n",
    "                'edu_seq_attention_mask': self.edu_seq_attention_mask[i],\n",
    "                'edu_seq_token_type_ids': self.edu_seq_token_type_ids[i],\n",
    "                'edu_labels' : self.label_edus[i],\n",
    "                'token_labels' : self.label_tokens[i]\n",
    "            \n",
    "        }\n",
    "        # return {'paragraph': self.paragraphs_tokenized[i], 'edus': self.edus_tokenized[i], 'labels': self.labels[i]}\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' this model should use BertModel to extract the embeddings of the paragraph\n",
    "    then do the following:\n",
    "        1. use EDU split to get the embeddings of each tokens of an EDU\n",
    "        2. represent the EDU as the average embedding of its member tokens\n",
    "        3. pass the EDU embedding to the classifier layer to make predictions\n",
    "        4. calculate the loss based on the predicted and gold EDU labels\n",
    "'''\n",
    "\n",
    "class BertForPhraseClassification(BertPreTrainedModel):\n",
    "\n",
    "    # _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
    "\n",
    "    def __init__(self, config, edu_sequence_length=50):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.num_labels = config.num_labels\n",
    "        self.edu_sequence_length = edu_sequence_length\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        classifier_dropout = (\n",
    "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
    "        )\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        self.classifier = nn.Linear(self.edu_sequence_length, config.num_labels)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        edu_seq_input_ids=None,\n",
    "        edu_seq_attention_mask=None,\n",
    "        edu_seq_token_type_ids=None,\n",
    "        edu_labels=None,\n",
    "        token_labels=None,\n",
    "    ):\n",
    "        # edu_outputs of size: batch_size(=16), msx edus in one paragraph (=50), and bert hidden layer size (=768)\n",
    "        edu_outputs = torch.zeros(edu_seq_input_ids.shape[0], self.edu_sequence_length, self.config.hidden_size)\n",
    "        for i in range(self.edu_sequence_length):\n",
    "            outputs = self.bert(edu_seq_input_ids[:, i, :], attention_mask=edu_seq_attention_mask[:, i, :], token_type_ids=edu_seq_token_type_ids[:, i, :])\n",
    "            print(outputs[1].shape, edu_outputs[i].shape, edu_outputs.shape)\n",
    "            edu_outputs[:, i, :] = outputs[1]\n",
    "\n",
    "        # outputs = self.bert(edu_seq_input_ids, attention_mask=edu_seq_attention_mask, token_type_ids=edu_seq_token_type_ids)\n",
    "        # outputs = outputs[1]\n",
    "        \n",
    "        edu_outputs = self.dropout(edu_outputs)\n",
    "        logits = self.classifier(edu_outputs)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            \n",
    "        output = (logits,) + outputs[2:]\n",
    "        return ((loss,) + output) if loss is not None else output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/tariq/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/tariq/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForPhraseClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForPhraseClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForPhraseClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForPhraseClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "paragraph_files, edus_files, labels_files = '../data/ets/para_text/*', '../data/ets/para_edu/*', '../data/ets/para_edu_label_all/*'\n",
    "argdata = ArgumentDataset(tokenizer, paragraph_files, edus_files, labels_files)\n",
    "edu_tag_model = BertForPhraseClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./',      \n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,  \n",
    "    save_steps=0, \n",
    "    do_train=True,\n",
    "    dataloader_drop_last=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=edu_tag_model,        \n",
    "    args=training_args,                \n",
    "    train_dataset=argdata,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 787\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 768]) torch.Size([50, 768]) torch.Size([16, 50, 768])\n",
      "torch.Size([16, 768]) torch.Size([50, 768]) torch.Size([16, 50, 768])\n",
      "torch.Size([16, 768]) torch.Size([50, 768]) torch.Size([16, 50, 768])\n",
      "torch.Size([16, 768]) torch.Size([50, 768]) torch.Size([16, 50, 768])\n",
      "torch.Size([16, 768]) torch.Size([50, 768]) torch.Size([16, 50, 768])\n",
      "torch.Size([16, 768]) torch.Size([50, 768]) torch.Size([16, 50, 768])\n",
      "torch.Size([16, 768]) torch.Size([50, 768]) torch.Size([16, 50, 768])\n",
      "torch.Size([16, 768]) torch.Size([50, 768]) torch.Size([16, 50, 768])\n",
      "torch.Size([16, 768]) torch.Size([50, 768]) torch.Size([16, 50, 768])\n",
      "torch.Size([16, 768]) torch.Size([50, 768]) torch.Size([16, 50, 768])\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "# trainer.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50, 128]),\n",
       " torch.Size([50, 128]),\n",
       " torch.Size([50, 128]),\n",
       " 50,\n",
       " 50,\n",
       " 1,\n",
       " 128)"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "argdata[0]['edu_seq_input_ids'].shape, argdata[0]['edu_seq_attention_mask'].shape, argdata[0]['edu_seq_token_type_ids'].shape, \\\n",
    "len(argdata[0]['edu_labels']), len(argdata[0]['token_labels']), \\\n",
    "argdata[0]['edu_labels'][0], len(argdata[0]['token_labels'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sketching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "argdata.paragraphs_tokenized.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "argdata.edus_tokenized[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6195, 2313, 3930, 2311, 2047, 3558, 4547, 6502, 2015, 2003, 6827],\n",
       " [1998, 1037, 2843, 1997, 3361, 4219, 2024, 2036, 3223, 1012],\n",
       " [3784, 2495, 2003, 1037, 5576],\n",
       " [2000, 2191, 2495, 9694, 2625, 3558, 5918, 1012],\n",
       " [2144,\n",
       "  6624,\n",
       "  1997,\n",
       "  3784,\n",
       "  2495,\n",
       "  2064,\n",
       "  2191,\n",
       "  2495,\n",
       "  6502,\n",
       "  2015,\n",
       "  5478,\n",
       "  2625,\n",
       "  10605,\n",
       "  3361,\n",
       "  2490],\n",
       " [2009, 2097, 2031, 1996, 3754],\n",
       " [2000, 2022, 3024, 2625, 6450, 2005, 2216],\n",
       " [2040, 2024, 2036, 13732, 1999, 5157, 1012]]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[tok_id for tok_id in edu if tok_id not in [0,101,102]] for edu in argdata.edus_tokenized[0]['input_ids']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 6195,\n",
       " 2313,\n",
       " 3930,\n",
       " 2311,\n",
       " 2047,\n",
       " 3558,\n",
       " 4547,\n",
       " 6502,\n",
       " 2015,\n",
       " 2003,\n",
       " 6827,\n",
       " 1998,\n",
       " 1037,\n",
       " 2843,\n",
       " 1997,\n",
       " 3361,\n",
       " 4219,\n",
       " 2024,\n",
       " 2036,\n",
       " 3223,\n",
       " 1012,\n",
       " 3784,\n",
       " 2495,\n",
       " 2003,\n",
       " 1037,\n",
       " 5576,\n",
       " 2000,\n",
       " 2191,\n",
       " 2495,\n",
       " 9694,\n",
       " 2625,\n",
       " 3558,\n",
       " 5918,\n",
       " 1012,\n",
       " 2144,\n",
       " 6624,\n",
       " 1997,\n",
       " 3784,\n",
       " 2495,\n",
       " 2064,\n",
       " 2191,\n",
       " 2495,\n",
       " 6502,\n",
       " 2015,\n",
       " 5478,\n",
       " 2625,\n",
       " 10605,\n",
       " 3361,\n",
       " 2490,\n",
       " 2009,\n",
       " 2097,\n",
       " 2031,\n",
       " 1996,\n",
       " 3754,\n",
       " 2000,\n",
       " 2022,\n",
       " 3024,\n",
       " 2625,\n",
       " 6450,\n",
       " 2005,\n",
       " 2216,\n",
       " 2040,\n",
       " 2024,\n",
       " 2036,\n",
       " 13732,\n",
       " 1999,\n",
       " 5157,\n",
       " 1012,\n",
       " 102]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "argdata.paragraphs_tokenized['input_ids'][0][:70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[293, 342, 363, 365, 412, 463, 470, 473, 483, 494, 583, 662]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = [[(i, j+1, len(line.rstrip().split('\\t'))) for j, line in enumerate(para_labels) if len(line.rstrip().split('\\t')) != 2] for i, para_labels in enumerate(labels)]\n",
    "[l[0][0] for l in L if len(l)>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "edus = open('../data/ets/para_edu/1-AbxwVc5Fvl-bX7-RBPA8fLHOghrgNifbzu0hYLtSRY_1.txt').readlines()\n",
    "text = open('../data/ets/para_text/1-AbxwVc5Fvl-bX7-RBPA8fLHOghrgNifbzu0hYLtSRY_1.txt').readlines()\n",
    "labels = open('../data/ets/para_edu_label_all/1-AbxwVc5Fvl-bX7-RBPA8fLHOghrgNifbzu0hYLtSRY_1.txt').readlines()\n",
    "para_labels = [{'edu': line.rstrip().split('\\t')[0], 'tokens': line.rstrip().split('\\t')[1]} for line in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81, 81, 81)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(''.join(text).split()), len(''.join(edus).split()), sum([len(line['tokens'].split()) for line in para_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3., 4., 5.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = torch.zeros(10, 5)\n",
    "seq[0] = torch.Tensor([1, 2, 3, 4, 5])\n",
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertConfig\n",
    "config = BertConfig()\n",
    "bert = BertModel(config, add_pooling_layer=False)\n",
    "# bert = bert.from_pretrained('bert-based-uncased')\n",
    "\n",
    "string_tok = tokenizer('I love movies')\n",
    "ids = torch.tensor(string_tok['input_ids']).unsqueeze(0)\n",
    "attn_mask = torch.tensor(string_tok['attention_mask']).unsqueeze(0)\n",
    "seg_ids = torch.tensor(string_tok['token_type_ids']).unsqueeze(0)\n",
    "\n",
    "res = bert(ids, attention_mask=attn_mask, token_type_ids=seg_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
