{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data imports\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from collections import Counter\n",
    "# model imports\n",
    "# similar to https://github.com/huggingface/transformers/blob/14e9d2954c3a7256a49a3e581ae25364c76f521e/src/transformers/models/bert/modeling_bert.py\n",
    "import logging\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss, HingeEmbeddingLoss\n",
    "\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers.models.bert.modeling_bert import BertEmbeddings, BertModel, BertPreTrainedModel, BertConfig\n",
    "from transformers.utils import logging\n",
    "\n",
    "from transformers.file_utils import ModelOutput\n",
    "from typing import Optional\n",
    "\n",
    "# logger = logging.get_logger(__name__)\n",
    "\n",
    "# Trainer imports\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each training instance consists of a paragraph, edu splits and edu labels\n",
    "\n",
    "class ArgumentDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, tokenizer, paragraph_files, edus_files, labels_files, max_len=256, max_edu_seq=50):\n",
    "        \n",
    "        self.max_len, self.max_edu_seq = max_len, max_edu_seq\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.paragraphs = [''.join(open(file).readlines()) for file in glob.glob(paragraph_files)]\n",
    "        self.edus = [open(file).readlines() for file in glob.glob(edus_files)]\n",
    "        self.labels = [open(file).readlines() for file in glob.glob(labels_files)]\n",
    "        self.label2id = {'B-claim': 1, 'I-claim': 2, 'B-premise': 3, 'I-premise': 4, 'O' : 0}\n",
    "        \n",
    "        ######\n",
    "        # filterout = [7, 24, 89, 231, 298, 348, 370, 373, 421, 473, 481, 485, 496, 508, 599, 680] # linux file order\n",
    "        filterout = [27, 99, 163, 183, 191, 194, 226, 239, 259, 271, 289, 377, 410, 582, 626, 656] # mac file order\n",
    "        for i in filterout[::-1]:\n",
    "            self.paragraphs.pop(i); self.edus.pop(i); self.labels.pop(i)\n",
    "        ######\n",
    "        \n",
    "        self.labels = [\n",
    "            [{'edu': line.rstrip().split('\\t')[0], 'tokens': line.rstrip().split('\\t')[1]} for line in para_labels]\n",
    "                      for para_labels in self.labels\n",
    "        ]\n",
    "        self.label_edus = [[0 for _ in range(self.max_edu_seq)] for _ in self.labels]\n",
    "        self.label_tokens = [[[0 for _ in range(self.max_len)] for _ in range(self.max_edu_seq)] for _ in self.labels]\n",
    "        \n",
    "        self.para_edu_splits = [' [EDU_SEP] '.join([line.rstrip() for line in para_edus]) for para_edus in self.edus]\n",
    "        self.para_edu_splits_tok = self.tokenizer(self.para_edu_splits, truncation=True, padding='max_length', max_length=self.max_len)\n",
    "                \n",
    "        for i, para_edus in enumerate(self.labels):\n",
    "            for j in range(min(self.max_edu_seq, len(self.labels[i]))):\n",
    "                self.label_edus[i][j] = self.label2id[self.labels[i][j]['edu']]\n",
    "                for k in range(min(self.max_len, len(self.labels[i][j]['tokens'].split()))):\n",
    "                    self.label_tokens[i][j][k] = self.label2id[self.labels[i][j]['tokens'].split()[k]]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return {'input_ids': self.para_edu_splits_tok['input_ids'][i],\n",
    "                'attention_mask': self.para_edu_splits_tok['attention_mask'][i],\n",
    "                'token_type_ids': self.para_edu_splits_tok['token_type_ids'][i],\n",
    "                'edu_labels' : self.label_edus[i],\n",
    "                'token_labels' : self.label_tokens[i]\n",
    "               }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer.add_special_tokens({'additional_special_tokens':['[EDU_SEP]']})\n",
    "\n",
    "paragraph_files, edus_files, labels_files = '../data/ets/para_text/*', '../data/ets/para_edu/*', '../data/ets/para_edu_label/*'\n",
    "argdata_old = ArgumentDataset(tokenizer, paragraph_files, edus_files, labels_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' this model should use BertModel to extract the embeddings of the paragraph\n",
    "    then do the following:\n",
    "        1. use EDU split to get the embeddings of each tokens of an EDU\n",
    "        2. represent the EDU as the average embedding of its member tokens\n",
    "        3. pass the EDU embedding to the classifier layer to make predictions\n",
    "        4. calculate the loss based on the predicted and gold EDU labels\n",
    "'''\n",
    "\n",
    "class BertForPhraseClassification(BertPreTrainedModel):\n",
    "\n",
    "    # _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
    "\n",
    "    def __init__(self, config, edu_sequence_length=50):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        self.num_labels = config.num_labels\n",
    "        self.edu_sep_id = 30522\n",
    "        self.edu_sequence_length = edu_sequence_length\n",
    "\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        # self.bert = BertModel(config)\n",
    "        classifier_dropout = (\n",
    "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
    "        )\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        # print(self.config)\n",
    "        self.init_weights()\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        edu_labels=None,\n",
    "        token_labels=None,\n",
    "    ):\n",
    "\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        sequence_output = outputs[0]\n",
    "        # print(input_ids.shape, outputs.shape)\n",
    "        edu_embeddings = self.get_edu_emb(input_ids, sequence_output)\n",
    "        print(edu_embeddings.shape)\n",
    "        \n",
    "        edu_embeddings = self.dropout(edu_embeddings)\n",
    "        logits = self.classifier(edu_embeddings)\n",
    "        print(logits.shape)\n",
    "\n",
    "        loss = None\n",
    "        if edu_labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), edu_labels.view(-1))\n",
    "            \n",
    "        output = (logits,) + outputs[2:]\n",
    "        return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "    \n",
    "    def get_edu_emb(self, input_ids, outputs, edu_seperator_id=30522):\n",
    "        'Returns a sequence of 50 EDUs (padded or truncated) per paragraph represented as the average embeddings of their tokens'\n",
    "        batch_size = outputs.shape[0]\n",
    "        batch_para_edu_avg_emb = torch.zeros(batch_size,  self.edu_sequence_length, self.config.hidden_size)\n",
    "        \n",
    "        # finding the \"[EDU_SEP]\" token in each paragraph given a batch of input_ids\n",
    "        # seperators[0] has paragraph id in a batch\n",
    "        # seperators[1] has index of \"[EDU_SEP]\" in all paragraphs\n",
    "        seperators = (input_ids == edu_seperator_id).nonzero(as_tuple=True)\n",
    "        \n",
    "        # getting the number of edus in each paragraph\n",
    "        edu_per_para, all_keys, i = [], range(batch_size), 0\n",
    "        for k, v in Counter([t.item() for t in seperators[0]]).items():\n",
    "            while k != all_keys[i] and i < len(all_keys):\n",
    "                edu_per_para.append(0); i+=1\n",
    "            edu_per_para.append(v); i+=1\n",
    "        \n",
    "        # calculating the average embeddings for each EDU\n",
    "        seperators_idx = 0\n",
    "        for i, edu_count_per_para in enumerate(edu_per_para):\n",
    "            prev_edu_sep = 0\n",
    "            for j in range(edu_count_per_para):\n",
    "                if j < self.edu_sequence_length:\n",
    "                    cur_edu_sep = seperators[1][seperators_idx].item()\n",
    "                    # print(i, j, prev_edu_sep, cur_edu_sep)\n",
    "                    assert input_ids[i][prev_edu_sep] in [101, edu_seperator_id]\n",
    "                    assert input_ids[i][cur_edu_sep] in [edu_seperator_id, 102]\n",
    "\n",
    "                    batch_para_edu_avg_emb[i][j] = torch.mean(outputs[i][prev_edu_sep+1:cur_edu_sep], dim=0)\n",
    "                    prev_edu_sep = cur_edu_sep\n",
    "\n",
    "                seperators_idx += 1;\n",
    "            \n",
    "            if j < self.edu_sequence_length -1:\n",
    "                # calculating embeddings of the last EDU that is between [EDU_SEP] and [SEP]\n",
    "                cur_edu_sep = (input_ids[i] == 102).nonzero(as_tuple=True)[0].item()\n",
    "                # print(i, j+1, prev_edu_sep, cur_edu_sep)\n",
    "                assert input_ids[i][prev_edu_sep] in [101, edu_seperator_id]\n",
    "                assert input_ids[i][cur_edu_sep] in [edu_seperator_id, 102] \n",
    "                batch_para_edu_avg_emb[i][j] = torch.mean(outputs[i][prev_edu_sep+1:cur_edu_sep], dim=0)\n",
    "            \n",
    "        return batch_para_edu_avg_emb\n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "783"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(argdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer.add_special_tokens({'additional_special_tokens':['[EDU_SEP]']})\n",
    "\n",
    "paragraph_files, edus_files, labels_files = '../data/ets/para_text/*', '../data/ets/para_edu/*', '../data/ets/para_edu_label/*'\n",
    "argdata = ArgumentDataset(tokenizer, paragraph_files, edus_files, labels_files)\n",
    "\n",
    "config = BertConfig.from_pretrained(\"bert-base-uncased\", num_labels=5)\n",
    "edu_tag_model = BertForPhraseClassification.from_pretrained(\"bert-base-uncased\", config=config)\n",
    "edu_tag_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./',      \n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,  \n",
    "    save_steps=0, \n",
    "    do_train=True,\n",
    "    dataloader_drop_last=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=edu_tag_model,        \n",
    "    args=training_args,                \n",
    "    train_dataset=argdata,\n",
    ")\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sketching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_files, edus_files, labels_files = '../data/ets/para_text/*', '../data/ets/para_edu/*', '../data/ets/para_edu_label/*'\n",
    "for pf, ef, lf in zip( glob.glob('../data/ets/para_text/*'), glob.glob('../data/ets/para_edu/*'), glob.glob('../data/ets/para_edu_label/*') ):\n",
    "    print('{}\\n{}\\n{}\\n\\n'.format(pf, ef, lf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([356]), torch.Size([356]), torch.Size([16, 512]))"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids_batch = torch.tensor(argdata.para_edu_splits_tok['input_ids'][:16])\n",
    "outputs, edu_seperator_id = torch.rand(16, 512, 768), 30522\n",
    "seperators = (input_ids_batch == edu_seperator_id).nonzero(as_tuple=True)\n",
    "\n",
    "edu_per_para = list(Counter([t.item() for t in seperators[0]]).values())\n",
    "\n",
    "seperators[0].shape, seperators[1].shape, input_ids_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[293, 342, 363, 365, 412, 463, 470, 473, 483, 494, 583, 662]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = [[(i, j+1, len(line.rstrip().split('\\t'))) for j, line in enumerate(para_labels) if len(line.rstrip().split('\\t')) != 2] for i, para_labels in enumerate(labels)]\n",
    "[l[0][0] for l in L if len(l)>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "edus = open('../data/ets/para_edu/1-AbxwVc5Fvl-bX7-RBPA8fLHOghrgNifbzu0hYLtSRY_1.txt').readlines()\n",
    "text = open('../data/ets/para_text/1-AbxwVc5Fvl-bX7-RBPA8fLHOghrgNifbzu0hYLtSRY_1.txt').readlines()\n",
    "labels = open('../data/ets/para_edu_label_all/1-AbxwVc5Fvl-bX7-RBPA8fLHOghrgNifbzu0hYLtSRY_1.txt').readlines()\n",
    "para_labels = [{'edu': line.rstrip().split('\\t')[0], 'tokens': line.rstrip().split('\\t')[1]} for line in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertConfig\n",
    "config = BertConfig()\n",
    "bert = BertModel(config, add_pooling_layer=False)\n",
    "# bert = bert.from_pretrained('bert-based-uncased')\n",
    "\n",
    "string_tok = tokenizer('I love movies')\n",
    "ids = torch.tensor(string_tok['input_ids']).unsqueeze(0)\n",
    "attn_mask = torch.tensor(string_tok['attention_mask']).unsqueeze(0)\n",
    "seg_ids = torch.tensor(string_tok['token_type_ids']).unsqueeze(0)\n",
    "\n",
    "res = bert(ids, attention_mask=attn_mask, token_type_ids=seg_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
